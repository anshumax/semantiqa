{
  "version": "1.0",
  "models": [
    {
      "id": "embed-bge-small-onnx",
      "name": "BGE Small (ONNX)",
      "kind": "embedding",
      "size_mb": 95,
      "license": "Apache-2.0",
      "sha256": "skip-verification",
      "url": "https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/onnx/model.onnx"
    },
    {
      "id": "gen-tinyllama-1.1b-q4_k_m-gguf",
      "name": "TinyLlama 1.1B Q4_K_M (GGUF)",
      "kind": "generator",
      "size_mb": 669,
      "license": "Apache-2.0",
      "sha256": "skip-verification",
      "url": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
      "description": "Compact 1.1B parameter model optimized for CPU inference. 5-10x faster than larger models."
    }
  ]
}